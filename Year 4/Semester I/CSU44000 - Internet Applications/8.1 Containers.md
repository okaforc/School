VMs are a good form of packaging but they take a hit on raw machine performance at the instruction level. They require each machine to be booted, which can take several minutes, and have its own native filesystem.

## Linux Containers
Linux Containers solved many of these problems by:
- using an OS-level virtualization method
- enabling running multiple isolated Linux systems (containers)
- running on a control host using a single Linux kernel

### Structure
At the core of the Linux OS is the **Kernel**. Various groups and companies have constructed components that go on top of this:
- Package managers (apt, yum)
- Windowing systems
- etc.
The mix of kernel and components are called **Distributions** or **Distros**. Popular ones include:
- Ubuntu
- RedHat
- Debian
- Arch (!!!!)

There are over 600 distros, roughly 500 of which are in active development.

## Docker
Docker was launched in 2011 from Y-Combinator, initially working on PaaS (Platform as a Service, similar to Google Docs). It used Linux Containers (LXCs) as a core component. This was replaced with Go in 2014.

### Docker Containers
A container is built-up using layers. It uses a copy-on-write system so similar containers do not cause duplicate layers. 

The Base Layer contains the Linux Kernel.
The next layer is likely on a distro (e.g., Ubuntu). 
Next is the key operating software such as Node.js.
Finally, there is the end-user application.

This all forms a container specification, which can easily be shipped with a container image.

### Structure
The target machine runs a Docker "host". This contains:
- a (usually) single Linux Kernel
- A set of container images, initially empty
- A set of running containers, initially empty
It is remotely controlled by a Docker client using a REST API. If asked to use a container image not in its set of images, it can retrieve it from a container registry.

### Dockerfile
Docker containers are described using a Dockerfile. 

- Starts with a Node.js (v20) image, 
- sets the working directory, 
- copies some files into the image,
- runs commands on the image,
- exposes port for traffic, and
- specifies a start-up command

```dockerfile
FROM node:20
# Create app directory 
WORKDIR /usr/src/app 

# Install app dependencies 
# A wildcard is used to ensure both package.json 
# AND package-lock.json are copied 
# where available (npm@5+) 
COPY package*.json ./ 

RUN npm install 
# If you are building your code for production 
# RUN npm ci --only=production 

# Bundle app source 
COPY . . 

EXPOSE 8080 
CMD [ "node", "server.js" ]
```

### Managing Container Run Environment - Volumes
The disk storage in containers is ephemeral, re-initialised each time the container is created. Using the Volume support, file system areas can be mounted onto the running instance. This will expose files to the container and starts a second container with the same mounted volumes as the first.

Resources can be allocated to containers to limit how much they can consume. The CPU usage of a container can be throttled. The scheduler divides available CPU power into 1024 time shares that can be allocated to individual containers. This only really matters when the CPU is busy. Swap space can also be constrained.

## Orchestration Tools
**Orchestration is the process of managing large complex configurations on running containers running on potentially very large clusters or multi-location cloud facilities.**

Multiple companies and groups have their own orchestration tools, such as:
- Swarm from Docker
- Fleet from CoreOS
- Apache Mesos
- Google Kubernetes (leading)
- Helios from Spotify

This area is still very much in flux.

### Microservices Architecture
MA splits entire the application into distinct "Services" that communicate using REST APIs or other similar comms mechanisms. These Services can be:
- in different languages
- use different technologies
- be owned by separate teams
- be independently testable
- easily scaled

### AWS Elastic Container Service
AWS ECS are Amazon's version of CaaS (Container as a Service).

First, you can setup a Cluster specifying VM sizes, numbers, memory, etc. When, you describe the container configuration as a "Task". You can use Elastic Load Balancers to share load across containers, or use a private container registry which gives more control from a security perspective. You can then launch and view the container, and pay for the underlying resources (vCPUs, etc.) used.

### AWS ECS for Kubernetes (EKS)
ECS also has a version made for EKS. Here, you:
- create a cluster
	- specify machine types, etc.
- create a kuberconfig file specifying the container layout
- deploy
- charged for resources used

### AWS Fargate
Fargate is another form of CaaS. You can define container configuration as a "Task", deploy on Fargate, and pricing is based on vCPU and GB memory used from the `docker pull` command.