[[qpf.pdf]]
13/02/23
## Looking at nodes in terms of their distance from the goal - Goal Set
1. General goal set
	$G_n\approx$ {nodes with distance n from G}
2. Goal set with distance 0 (just the goal!)
	$G_0= G$
3. Goal set n+1
	- $G_{n+1} := \{s\ |\ (\exists \ s'\  \in \  G_n),\ \  arc(s,s')\} - \cup^n_{i=1}\ G_i$
	- All those which have an arc from an element of $G_n$, subtracting those that are already in a goal set

### Example
![[Pasted image 20230213093723.png]]
Assume V is the goal

$\{V\}_0= \{V\}$
$\{V\}_1= \{SA,NSW\}$
$\{V\}_2= \{WA,NT,Q\}$
$\{V\}_\infty= \{T\}$

## From the distance we are trying to minimize, to finding the reward
### 1. Distance To Minimize - $d_G(s)$
- `n` if the state is in goal set `n` (`n` distance from goal node)
- Otherwise it is infinity![[Pasted image 20230213094222.png]]
### 2. Some form of reward - $\delta_G(s)$
Refined to
- 1 if the state *is* a goal node
- 0 if the state is not a goal node
- This is a *reward* from 1 to 0 (where 1 matches with distance being 0, and 0 matches with anything up to infinity)
![[Pasted image 20230213094235.png]]

### 3. Reward to Maximize - $r_G(s)$
Refined further
- Reward halves every time we step back (starting at a goal)
- If s is reachable from G, reward is $\frac{1}{2^n}$ (halfs from 1 the farther away we get)
- Otherwise (presumably including if the goal set is infinity), the reward is 0
![[Pasted image 20230213094331.png]]

Reward of s is half the reward of s' if there is an arc from s to s' and the distance from G to s' is less than the distance from G to s:![[Pasted image 20230419125306.png]]


## Rewards for looking ahead (heuristic)
1. The heuristic for starting off: (1 if it is in G, 0 otherwise) ![[Pasted image 20230419130319.png]]
2. The heuristic used for every other check
	- First, add the reward for if the state is a goal node. This is either 0 or 1
	- Then, add the *maximum* potential reward from every possible node connected to the node![[Pasted image 20230419130414.png]]
Note this uses the arc predicate 
- $arc_=$ encodes either moving nodes or staying still (rest)
- $arc_= \Leftrightarrow arc(s,s')$ or $s=s'$

### Messing around with this for what I assume are maths reasons
#### Heuristic for the goal node
When s $\in G_0$ (is a goal node), the reward is
![[Pasted image 20230419131339.png]]
![[Pasted image 20230419131409.png]]
So, when s is a goal node, 
$$H_{n+1}(s) = 2(1-2^{-(n+1)})$$

#### Heuristic for non-goal node
In this case: ![[Pasted image 20230419131713.png]]
(where s is not a goal node, half the reward)
![[Pasted image 20230419131738.png]]
#### Eventually gives
![[Pasted image 20230419131834.png]]As the number of nodes approaches infinity, the reward to get to s is ???, we just did backward reduction

![[Pasted image 20230213094331.png]]


## Our heuristic - $H=lim_{n\rightarrow \infty}H_n$
![[Pasted image 20230419132100.png]]
This is a foolproof heuristic for the shortest solution. This gives, as we want:
`Frontier = [Head|Tail] with H(Head) ≥ H(s) for all s in Tail`

### Arc costs other than 1?
What if arcs have different costs? 

#### 1. Modify $\delta_G(s)$  to $Q_0(s,s')$ 
Modify $\delta_G(s)$ to $Q_0(s,s')$ - includes arc/rest, s, s'!
1. If s and s' are the same node, which is a goal node
	- Heuristic equals 1
2. If there is an arc directly from s to s'
	- Heuristic equals minus the cost of that arc
	- (Minus as we are trying to maximize reward, and a cost is bad)
3. If there is not an arc between s and s'
	- Give the worst possible negative value
	- Essentially $-\infty$
![[Pasted image 20230419133212.png]]

#### 2. Modify $H_{n+1}(s)$ to $Q_{n+1}(s,s')$
- Modify $H_{n+1}(s)$ to $Q_{n+1}(s,s')$ - includes arc/rest, s, s'!
- This happens when there is a lookahead
- Finding the best move possible from s
- Essentially the same as $H_{n+1}$ ![[Pasted image 20230419160805.png]]

### The maths behind this - Discounted Rewards
- $\gamma$ is the discounting rate
- $0 \leq \gamma <1$

#### 1. Initial  $\gamma$ discounted value - $V$
- Assume immediate rewards $r_1, r_2, r_3, ...$ at are gotten times 1, 2, 3, ...
- Then, there is a $\gamma$ discounted value of ![[Pasted image 20230419161412.png]]
#### 2. Introduce $V_t$
Then, say $V_t$ is the $\gamma$ discounted value at time step `t` (using backward induction)![[Pasted image 20230419161817.png]]
#### 3. Rewrite $V$
Using $V_t$, the $\gamma$ discounted value $V$ can be rewritten as this (using the geometric series) ![[Pasted image 20230419161858.png]]
#### 4. Bounds of $V_t$
$V_t$ is bound based on the bounds of $r_i$, the reward at a particular time step
![[Pasted image 20230419162334.png]]

## Our heuristic isn't working - examples
Reminder of our heuristic:
![[Pasted image 20230419162535.png]]
Q is estimate of how desirable it is to make a move! (foolproof heuristic)

### Example 1 - Solution not chosen
![[Pasted image 20230419162619.png]]

1. Our starting point is s

#### Q(g,g) = 2
$$Q(g,g) = Q_0(g,g)+\frac{1}{2}max \{Q(g,s'') \ | \ arc_=(g,s'')\}$$
- $Q_0(g,g)$ = 1
- The only possible action is rest in g => $$Q(g,g) = 1+ \frac{1}{2}Q(g,g)$$
- Bit of algebra leads to answer $$\frac{1}{2}Q(g,g) = 1$$

#### Q(s,g) = -3
$$Q(s,g) = Q_0(s,g)+\frac{1}{2}max \{Q(g,s'') \ | \ arc_=(g,s'')\}$$
- $Q_0(s,g)$ = -cost(s,g) = -4
- Only possible action in g is rest => $$Q(s,g) = -4+\frac{1}{2}Q(g,g)$$
- We already know Q(g,g), so we get our answer

#### Q(s,a) = 
$$Q(s,a) = Q_0(s,a)+\frac{1}{2}max \{Q(a,s'') \ | \ arc_=(a,s'')\}$$
- $Q_0(s,a)$ = -cost(s,a) = -1
- Only good action in a is move to b => $$Q(s,a) = -1+\frac{1}{2}Q(a,b)$$
- Now we need to calculate Q(a,b)

#### Q(a,b) = 
$$Q(a,b) = Q_0(a,b)+\frac{1}{2}max \{Q(b,s'') \ | \ arc_=(b,s'')\}$$
- $Q_0(a,b)$ = -cost(a,b) = -1
- Only good action is to move to s $$Q(a,b) = -1+\frac{1}{2}Q(b,s)$$
- Now we need to calculate Q(b,s)

#### Q(b,s) = 
$$Q(b,s) = Q_0(b,s)+\frac{1}{2}max \{Q(s,s'') \ | \ arc_=(s,s'')\}$$
- $Q_0(b,s)$ = -cost(b,s) = -1
- Two possible actions $$Q(b,s) = -1+\frac{1}{2}max \{Q(s,a),-3\}$$
#### What now? 
Now we have two cases, Q(s,a) < -3, and Q(s,a) >= -3.

##### Case 1: Q(s,a) < -3
We do this case first, as it will give us an actual value to work with.
In this case, 
1. From the equation for Q(b,s)$$Q(b,s) = -1+\frac{-3}{2} = -2.5$$
2. From the equation for Q(a,b)$$Q(a,b) = -1+\frac{1}{2}\times\frac{-5}{2} = -2.25$$
3. From the equation for Q(s,a)$$Q(s,a) = -1+\frac{1}{2}\times-2.25=-2.125$$
4. This is a contradiction, as -2.125 is larger than -3.

Therefore we can assume case 2 is true.

##### Case 2: Q(s,a) >= -3
1. From the equation for Q(b,s) $$Q(b,s) = -1+\frac{1}{2}Q(s,a)$$
2. From the equation for Q(a,b) $$Q(a,b) = -1+\frac{1}{2}(-1+\frac{1}{2}Q(s,a))=-1.5 + \frac{1}{4}Q(s,a)$$
3. From the equation for Q(s,a) $$Q(s,a) = -1+\frac{1}{2}(-1.5 + \frac{1}{4}Q(s,a))=-1.75+\frac{1}{8}Q(s,a)$$
4. Rearranging... $$\frac{7}{8}Q(s,a) = -1.75, Q(s,a)=2$$
Then the final values are
1. Q(s,a) = -2
2. Q(a,b) = -2
3. Q(b,s) = -2

#### Starting from s
Our options are 
1. Go to g, with a Q value of -3
2. Go to a, with a Q value of -2

This means it will constantly loop, going from s to a to b to s instead of getting to goal node!


### Example 2 - Costlier solution chosen
![[Pasted image 20230419170747.png]]

#### Q(g,g) = 2
$$Q(g,g)=Q_0(g,g)+\frac{1}{2}max\{Q(g,s')|arc_=(g,s')\}$$
- $Q_0(g,g)$=1
- the only option from g is to rest => $$Q(g,g)=1+\frac{1}{2}Q(g,g)$$
- Bit of algebra gives = 2


#### Q(s,g) = -5
$$Q(s,g)=Q_0(s,g)+\frac{1}{2}max\{Q(g,s')|arc_=(g,s')\}$$
- $Q_0(s,g)$=-6
- Only option from g is to rest => $$Q(s,g)=-6+\frac{1}{2}Q(g,g)$$
- We know Q(g,g), so $$Q(s,g)=-6+\frac{2}{2}=-5$$

#### Q(a,g) = -4
$$Q(a,g)=Q_0(a,g)+\frac{1}{2}max\{Q(g,s')|arc_=(g,s')\}$$
- $Q_0(a,g)$=-5
- Only option from g is to rest so => $$Q(a,g)=-5+\frac{2}{2}=-4$$

#### Q(s,a) =-4
$$Q(s,a)=Q_0(s,a)+\frac{1}{2}max\{Q(a,s')|arc_=(a,s')\}$$

- $Q_0(s,a)$=-2
- Best option from a is to go to g so $$Q(s,a)=-2+\frac{1}{2}Q(a,g)$$
- We know Q(a,g) $$Q(s,a)=-2+\frac{-4}{2}=-4$$
#### What happens?
Go from s to a to g instead of straight to g

This isn't good, so we raise the reward

## Raising the reward - from $Q_0(s,s')$ to $R(s,s')$
Adjust $Q_0(s,s')$ to
![[Pasted image 20230217143644.png]]
- `r` should be high enough to offset the costs of reaching a goal
### Example `r` value
![[Pasted image 20230419173403.png]]
This works for solutions with
1. n-1 arcs (n states)
2. arcs costing ≤ c (c is max arc cost)

### Labeling max{Q(s,s')|arc(s,s')} as V(s)
We define V(s)
![[Pasted image 20230419173654.png]]
#### Qualities of V(s)
For 
1. `0 ≤ i < n`
2. s' $\in\  G_i$ 
3. `arc(s,s')`
(essentially just all valid arcs)

We have:
1. The reward is a sufficient size (This is the `r` value!) ![[Pasted image 20230419174237.png]]
2. Some other qualities... ![[Pasted image 20230419174257.png]]

## Recap
###### Notes I took at the time...
All letters are heuristics
R is immediate reward
V and Q are evaluations of nodes and states

### Actual Recap
1. From node s, find path to goal via s' maximising![[Pasted image 20230419174506.png]]
2. This has discount $\frac{1}{2}$ on future V(s') 
3. This is as opposed to ![[Pasted image 20230419174601.png]]
4. What we are doing here is trading 
	1. Minimum cost guarantee FOR
	2. Cost-benefit analysis with a 50% chance of survival/doom
5. Exploring more!