[[mdp.pdf]]

## A generalisation 
### **What we've been working with**
Given a specification ***R of immediate rewards*** after particular actions, calculate the ***return Q*** of particular actions over time via
$$Q=lim_{n\rightarrow \infty}Q_n$$

### How we go from $Q_n$ to $Q_{n+1}$ (1)
![[Pasted image 20230220092851.png]]
Maximizing the reward!

### Generalising the $\frac{1}{2}$ to $\gamma$, and adding an action (2)
![[Pasted image 20230220092955.png]]

a' is an action!

Making it non-deterministic, as action does not give answer

$\alpha$ is probability! - probability of actually getting from s to a (learned something!), vs probability that learn nothing, learning rate

### Comparing the previous 2
(1) is (2) less generally:
1. Action $a$ always resulting in $s'$ (deterministically)
2. $\alpha$=1
3. $\gamma = \frac{1}{2}$

![[Pasted image 20230220093255.png]]

s' is learned!

## Markov Decision Process
An MDP is a 5-tuple $\langle S, A, p, r, \gamma \rangle$ consisting of
1. Finite set S of states s, s', ...
2. Finite set A of actions a, a, ....
3. Function p: $S\times A \times S \rightarrow$ [0,1]
$p(s,a,s')$ = prob($s'|s,a$) = how probable is s' after doing a at s
$$\Sigma_{s'}p(s,a,s')=1 \ for \ all \ a \in A, \ s \in S$$
4. Function r:  $S\times A \times S \rightarrow \mathbb{R}$
$r(s,a,s')$ = immediate reward at $s'$ after $a$ is done at $s$

6. Discount factor $\gamma \in$ [0,1]

Missing is the policy $\pi: S \rightarrow A$ (what to do at s)

A behaviour that *continues*, reactive. Does not finish! (not like turing machine)

### Exercise/Example
Sam is either fit or unfit 
$S = {fit, unfit}$ 

and has to decide whether to exercise or relax 
$A = {exercise, relax}$

Need tables for the actions - If fit and exercises, will be fit with probability .99, reward 8.

$p(s,a,s')$ (*depend on the resulting state*) and $r(s,a,s')$ (*do not depend on the resulting state*, just action and starting state) are $a$-table entries for row $s$, col $s'$:
![[Pasted image 20230220094739.png]]

Now, fill in the rest! Assuming: 
1. Probability depends on the resulting state 
2. Immediate reward does not depend on resukting state
3. These are all possible states (probabilities add to 1)
![[Pasted image 20230220094837.png]]


### Example - Grid World
![[Pasted image 20230224142842.png]]

### Policy for an MDP
Given state *s*, pick action *a* that maximizes return:
![[Pasted image 20230224142946.png]]
for a $V$ tied to $Q$ via the policy $\pi : S\rightarrow A$ 
$$V_\pi(s) := Q(s,\pi(s))$$

#### Example of a policy - greedy Q-policy from above exercises
Just maximises it!
Policy:
$$\pi(s) := arg \ max_a Q(s,a)$$

Gives:
$$Q(s,a)= \Sigma_{s'} p(s,a,s')(r(s,a,s'))+\gamma \ max_{a'} Q(s',a')$$
Note that this is confusing as $Q$ is on both sides! How to calculate: iteration/indices:


### Value iteration
Mutual recursion between
1. $Q/V$ (value of an action/state)
2. $\pi$ (what to do at a state)

$$lim_{n\rightarrow \infty}q_n = Q$$

From the iterations
$$q_0(s,a):=\Sigma_{s'}p(s,a,s')r(s,a,s')$$
$$q_{n+1}(s,a):=\Sigma_{s'}p(s,a,s')(r(s,a,s')+\gamma \ max_{a'} q_n(s',a'))$$

If it's deterministic (no probability)
![[Pasted image 20230224144300.png]]


### Deterministic actions and Absorbing states (game over)
1. Fix an MDP with minimum immediate reward m (r function)
2. An action is *s-deterministic* if $p(s,a,s')=1$ for some state s
3. A state s is *absorbing* if $p(s,a,s')=1$ for *every* action $a$, where
![[Pasted image 20230224144506.png]]
4. A state s is a *sink* if it is absorbing and $r(s,a,s)=m$ for all $a$
5. An action a is an *s-drain* if for some sink $s'$
![[Pasted image 20230224144630.png]]

Now, let
$$A(s) := \{a \in A | \ a \ is \ not \ s-drain\}$$
So, if $A(s)\neq \emptyset$ 
![[Pasted image 20230224144854.png]]

### Arcs & Goals as a deterministic MDP ($p \in \{0,1\}$)
![[Pasted image 20230224144945.png]]


27/02/23
## Example
[[q4.pdf]]
![[Pasted image 20230227091219.png]]

Old determinisitic version, just with value 4 for goal state so that it stops looping (coincidence that its the same as the cost)!
![[Pasted image 20230227091245.png]]

### Value iteration
Calculating Qs by starting at $Q_0$, where Q0 is essentially r (reward!)

From s, move to a or g?
![[Pasted image 20230227091355.png]]
n=0: choose a


![[Pasted image 20230227091508.png]]
n=1: choose a

![[Pasted image 20230227091713.png]]
n=2: choose g

Choosing g will be preferable from then on


## Example 2
![[Pasted image 20230227092305.png]]

### Code
```prolog
% 3-node graph in p 20/26 of qpf.pdf
%
%| ?- show(5,0.5).
% (s,a)-2 (s,g)-6 (a,g)-5
% (s,a)-4.5 (s,g)-5.5 (a,g)-4.5
% (s,a)-4.25 (s,g)-5.25 (a,g)-4.25
% (s,a)-4.125 (s,g)-5.125 (a,g)-4.125
% (s,a)-4.0625 (s,g)-5.0625 (a,g)-4.0625
% (s,a)-4.03125 (s,g)-5.03125 (a,g)-4.03125
% true
%
% for optimal soln, set gamma= 0.9
% | ?- show(5,0.9).
% (s,a)-2 (s,g)-6 (a,g)-5
% (s,a)-6.5 (s,g)-5.1 (a,g)-4.1
% (s,a)-5.6899999999999995 (s,g)-4.29 (a,g)-3.29
% (s,a)-4.961 (s,g)-3.561 (a,g)-2.561
% (s,a)-4.3049 (s,g)-2.9049 (a,g)-1.9049
% (s,a)-3.71441 (s,g)-2.3144099999999996 (a,g)-1.3144099999999996
% true


/** <examples>

?- show(5,0.5).

?- show(9,0.9).

*/


:- dynamic qRes/5.
q(0,s,a,-2).   q(0,a,g,-5).    q(0,g,g,1).    q(0,s,g,-6).
v(X,a,V,G) :- q(X,a,g,V,G).
v(X,g,V,G) :- q(X,g,g,V,G).
q(X,S,A,Q,G) :-  qRes(X,S,A,Q,G),!.
q(0,S,A,Q,G) :- q(0,S,A,Q), assert(qRes(0,S,A,Q,G)).
q(s(X),S,A,Q,G) :-  q(0,S,A,Q0), v(X,A,V,G),
                    Q is Q0 + G*V,
                    assert(qRes(s(X),S,A,Q,G)).
show(X,G) :-  q(0,s,a,Q,G), write('(s,a)'), write(Q),
              q(0,s,g,Qq,G), write(' (s,g)'), write(Qq),
              q(0,a,g,Qa,G), write(' (a,g)'), write(Qa), nl,
              mkSu(X,Xs), show(0,Xs,G).
show(X,X,_).
show(X,N,G) :- q(s(X),s,a,Q,G), write('(s,a)'), write(Q),
               q(s(X),s,g,Qg,G), write(' (s,g)'), write(Qg),
               q(s(X),a,g,Qa,G), write(' (a,g)'), write(Qa), nl,
               show(s(X),N,G).
mkSu(N,S) :- mkSu(N,0,S).
mkSu(0,X,X).
mkSu(N,X,Y) :- N>0, M is N-1, mkSu(M,s(X),Y).
```

This eventually normalises at -4,-5