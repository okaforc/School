[[mdp2.pdf]]
27/02/23
## Recall MDP example - now with $\gamma$ =0.9
![[Pasted image 20230227092021.png]]

$q_0(s,a) := p(s,a,fit)r(s,a,fit)+p(s,a,unfit)r(s,a,unfit)$
$V_n := max(q_n(s,exercise),q_n(s,relax))$ - desirability of being in a certain state
$Q_{n+1}(s,a) := q_0(s,a) + .9(p(s,a,fit)V_n(fit)+p(s,a,unfit)V_n(unfit))$



$\gamma$ is the probability of paying attention to the future (rather than just the immediate present)

### This gives:
For $Q_0,Q_1,Q_2$
![[Pasted image 20230227092808.png]]

## Temporal difference (TD)
![[Pasted image 20230227092858.png]]

## Q-Learning
Values derived from rewards *and* states, learning from experience/environment (actions etc)

Assume $v_{k+1}$ is derived from $r_{k+1}$, $s_{k+1}$, observed sequentially
![[Pasted image 20230227093259.png]]

Then, 
$$v_{k+1} := r_{k+1}+ \gamma \ max_a Q_k(s_{k+1},a)$$
![[Pasted image 20230227093539.png]]
choosing between new value and not, $v_{k+!}$ from environment
Choosing action which makes most sense from next state,

alpha gotten from previous experience, learning rate frompast
gamma is discounting, future rewards

![[Pasted image 20230227094419.png]]

### Averages to

note that Qk is now a *table* of values (state and action) rather than what we've previously been working with as just a sequence of values
![[Pasted image 20230227094534.png]]

A bunch of this is irrelevant from our pov

### MDP is one experience at a time
![[Pasted image 20230313091824.png]]

Continue transitions until any future reward is 0, so there's no reason to move


### Exploration-exploitation tradeoff
This is discussion of how we choose action a!!!!
![[Pasted image 20230313092109.png]]

exploration is trying your chances, try different actions
exploitation is using information you already know

Just means it's a function unique outputs:
![[Pasted image 20230313092342.png]]

#### Example of non-detereministic $\pi$
![[Pasted image 20230313092402.png]]
introduce randomness

epsilon is the choice of the action we're gonna try, level of exploration (from 0-1)
epsilon 0 is argmaxing, exploitation
epsilon 1 is random choice, exploration

random choice with these probabilities, random number generator between 1 and n

#### Another example - SARSA
IGNORE FOR EXAM, NOT ACTUALLY EXPLAINED
![[Pasted image 20230313092618.png]]